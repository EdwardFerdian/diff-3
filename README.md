# DIFF-3
**3D echo** synthetic data generation with **paired labels**. 
This is a PyTorch implementation of the paper submitted to SASHIMI workshop, MICCAI 2023.

## Sample data
Comparison between real and synthetic images+labels generated by DIFF-3

<p align="center">
    <img src="./images/fig_samples.PNG">
</p>

## How To

### Requirements
A conda environment can be created and activated with 
  
    ...

### Training

#### Prepare training data
Prepare the data by resizing it to 160x160x128. Images are normalized.
  
    ...

#### Train VAE
Train VAE for 200 epochs with KL_weight adjusted from 1e-6 to 1 after 100 epochs
        
        ...


#### Train DDPM
Train DDPM for 50000 iterations
        
        ...


### Inference

        ...






## Network overview
DIFF-3 is an unconditioned Latent Diffusion Model (LDM) consisting of two stages:

1. Variational Auto Encoder (VAE)
2. Denoising Diffusion Probabilistic Model (DDPM)

The code is modified based on the original implementation from [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion) github, and adjusted to allow for 3D images+labels.

The image below shows the overview of the VAE network.
<p align="center">
    <img src="./images/fig_network.PNG">
</p>